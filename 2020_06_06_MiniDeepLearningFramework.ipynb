{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020_06_06-MiniDeepLearningFramework.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae_icQdRaiL6",
        "colab_type": "text"
      },
      "source": [
        "#This notebook is dedicated to the creation of a Mini-Deep Learning Framework (Based on *Grokking deep learning* <-Great book, check it out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEjncQqlmqN0",
        "colab_type": "text"
      },
      "source": [
        "First we will write the whole code so we can test the functions later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YfiwHSlVnzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Tensor (object):\n",
        "    \n",
        "    def __init__(self,data,\n",
        "                 autograd=False,\n",
        "                 creators=None,\n",
        "                 creation_op=None,\n",
        "                 id=None):\n",
        "        \n",
        "        self.data = np.array(data)\n",
        "        self.autograd = autograd\n",
        "        self.grad = None\n",
        "\n",
        "        if(id is None):\n",
        "            self.id = np.random.randint(0,1000000000)\n",
        "        else:\n",
        "            self.id = id\n",
        "        \n",
        "        self.creators = creators\n",
        "        self.creation_op = creation_op\n",
        "        self.children = {}\n",
        "        \n",
        "        if(creators is not None):\n",
        "            for c in creators:\n",
        "                if(self.id not in c.children):\n",
        "                    c.children[self.id] = 1\n",
        "                else:\n",
        "                    c.children[self.id] += 1\n",
        "\n",
        "    def all_children_grads_accounted_for(self):\n",
        "        for id,cnt in self.children.items():\n",
        "            if(cnt != 0):\n",
        "                return False\n",
        "        return True \n",
        "        \n",
        "    def backward(self,grad=None, grad_origin=None):\n",
        "        if(self.autograd):\n",
        " \n",
        "            if(grad is None):\n",
        "                grad = Tensor(np.ones_like(self.data))\n",
        "\n",
        "            if(grad_origin is not None):\n",
        "                if(self.children[grad_origin.id] == 0):\n",
        "                    return\n",
        "                    print(self.id)\n",
        "                    print(self.creation_op)\n",
        "                    print(len(self.creators))\n",
        "                    for c in self.creators:\n",
        "                        print(c.creation_op)\n",
        "                    raise Exception(\"cannot backprop more than once\")\n",
        "                else:\n",
        "                    self.children[grad_origin.id] -= 1\n",
        "\n",
        "            if(self.grad is None):\n",
        "                self.grad = grad\n",
        "            else:\n",
        "                self.grad += grad\n",
        "            \n",
        "            # grads must not have grads of their own\n",
        "            assert grad.autograd == False\n",
        "            \n",
        "            # only continue backpropping if there's something to\n",
        "            # backprop into and if all gradients (from children)\n",
        "            # are accounted for override waiting for children if\n",
        "            # \"backprop\" was called on this variable directly\n",
        "            if(self.creators is not None and \n",
        "               (self.all_children_grads_accounted_for() or \n",
        "                grad_origin is None)):\n",
        "\n",
        "                if(self.creation_op == \"add\"):\n",
        "                    self.creators[0].backward(self.grad, self)\n",
        "                    self.creators[1].backward(self.grad, self)\n",
        "                    \n",
        "                if(self.creation_op == \"sub\"):\n",
        "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
        "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
        "\n",
        "                if(self.creation_op == \"mul\"):\n",
        "                    new = self.grad * self.creators[1]\n",
        "                    self.creators[0].backward(new , self)\n",
        "                    new = self.grad * self.creators[0]\n",
        "                    self.creators[1].backward(new, self)                    \n",
        "                    \n",
        "                if(self.creation_op == \"mm\"):\n",
        "                    c0 = self.creators[0]\n",
        "                    c1 = self.creators[1]\n",
        "                    new = self.grad.mm(c1.transpose())\n",
        "                    c0.backward(new)\n",
        "                    new = self.grad.transpose().mm(c0).transpose()\n",
        "                    c1.backward(new)\n",
        "                    \n",
        "                if(self.creation_op == \"transpose\"):\n",
        "                    self.creators[0].backward(self.grad.transpose())\n",
        "\n",
        "                if(\"sum\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.expand(dim,\n",
        "                                                               self.creators[0].data.shape[dim]))\n",
        "\n",
        "                if(\"expand\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.sum(dim))\n",
        "                    \n",
        "                if(self.creation_op == \"neg\"):\n",
        "                    self.creators[0].backward(self.grad.__neg__())\n",
        "                    \n",
        "                if(self.creation_op == \"sigmoid\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
        "                \n",
        "                if(self.creation_op == \"tanh\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
        "                \n",
        "                if(self.creation_op == \"index_select\"):\n",
        "                    new_grad = np.zeros_like(self.creators[0].data)\n",
        "                    indices_ = self.index_select_indices.data.flatten()\n",
        "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
        "                    for i in range(len(indices_)):\n",
        "                        new_grad[indices_[i]] += grad_[i]\n",
        "                    self.creators[0].backward(Tensor(new_grad))\n",
        "                    \n",
        "                if(self.creation_op == \"cross_entropy\"):\n",
        "                    dx = self.softmax_output - self.target_dist\n",
        "                    self.creators[0].backward(Tensor(dx))\n",
        "                    \n",
        "    def __add__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data + other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"add\")\n",
        "        return Tensor(self.data + other.data)\n",
        "\n",
        "    def __neg__(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data * -1,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"neg\")\n",
        "        return Tensor(self.data * -1)\n",
        "    \n",
        "    def __sub__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data - other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"sub\")\n",
        "        return Tensor(self.data - other.data)\n",
        "    \n",
        "    def __mul__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data * other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"mul\")\n",
        "        return Tensor(self.data * other.data)    \n",
        "\n",
        "    def sum(self, dim):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.sum(dim),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sum_\"+str(dim))\n",
        "        return Tensor(self.data.sum(dim))\n",
        "    \n",
        "    def expand(self, dim,copies):\n",
        "\n",
        "        trans_cmd = list(range(0,len(self.data.shape)))\n",
        "        trans_cmd.insert(dim,len(self.data.shape))\n",
        "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
        "        \n",
        "        if(self.autograd):\n",
        "            return Tensor(new_data,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"expand_\"+str(dim))\n",
        "        return Tensor(new_data)\n",
        "    \n",
        "    def transpose(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.transpose(),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"transpose\")\n",
        "        \n",
        "        return Tensor(self.data.transpose())\n",
        "    \n",
        "    def mm(self, x):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.dot(x.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self,x],\n",
        "                          creation_op=\"mm\")\n",
        "        return Tensor(self.data.dot(x.data))\n",
        "    \n",
        "    def sigmoid(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sigmoid\")\n",
        "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
        "\n",
        "    def tanh(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(np.tanh(self.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"tanh\")\n",
        "        return Tensor(np.tanh(self.data))\n",
        "    \n",
        "    def index_select(self, indices):\n",
        "\n",
        "        if(self.autograd):\n",
        "            new = Tensor(self.data[indices.data],\n",
        "                         autograd=True,\n",
        "                         creators=[self],\n",
        "                         creation_op=\"index_select\")\n",
        "            new.index_select_indices = indices\n",
        "            return new\n",
        "        return Tensor(self.data[indices.data])\n",
        "    \n",
        "    def softmax(self):\n",
        "        temp = np.exp(self.data)\n",
        "        softmax_output = temp / np.sum(temp,\n",
        "                                       axis=len(self.data.shape)-1,\n",
        "                                       keepdims=True)\n",
        "        return softmax_output\n",
        "    \n",
        "    def cross_entropy(self, target_indices):\n",
        "\n",
        "        temp = np.exp(self.data)\n",
        "        softmax_output = temp / np.sum(temp,\n",
        "                                       axis=len(self.data.shape)-1,\n",
        "                                       keepdims=True)\n",
        "        \n",
        "        t = target_indices.data.flatten()\n",
        "        p = softmax_output.reshape(len(t),-1)\n",
        "        target_dist = np.eye(p.shape[1])[t]\n",
        "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
        "    \n",
        "        if(self.autograd):\n",
        "            out = Tensor(loss,\n",
        "                         autograd=True,\n",
        "                         creators=[self],\n",
        "                         creation_op=\"cross_entropy\")\n",
        "            out.softmax_output = softmax_output\n",
        "            out.target_dist = target_dist\n",
        "            return out\n",
        "\n",
        "        return Tensor(loss)\n",
        "        \n",
        "    \n",
        "    def __repr__(self):\n",
        "        return str(self.data.__repr__())\n",
        "    \n",
        "    def __str__(self):\n",
        "        return str(self.data.__str__())  \n",
        "\n",
        "class Layer(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.parameters = list()\n",
        "        \n",
        "    def get_parameters(self):\n",
        "        return self.parameters\n",
        "\n",
        "    \n",
        "class SGD(object):\n",
        "    \n",
        "    def __init__(self, parameters, alpha=0.1):\n",
        "        self.parameters = parameters\n",
        "        self.alpha = alpha\n",
        "    \n",
        "    def zero(self):\n",
        "        for p in self.parameters:\n",
        "            p.grad.data *= 0\n",
        "        \n",
        "    def step(self, zero=True):\n",
        "        \n",
        "        for p in self.parameters:\n",
        "            \n",
        "            p.data -= p.grad.data * self.alpha\n",
        "            \n",
        "            if(zero):\n",
        "                p.grad.data *= 0\n",
        "\n",
        "\n",
        "class Linear(Layer):\n",
        "\n",
        "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.use_bias = bias\n",
        "        \n",
        "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
        "        self.weight = Tensor(W, autograd=True)\n",
        "        if(self.use_bias):\n",
        "            self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
        "        \n",
        "        self.parameters.append(self.weight)\n",
        "        \n",
        "        if(self.use_bias):        \n",
        "            self.parameters.append(self.bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if(self.use_bias):\n",
        "            return input.mm(self.weight)+self.bias.expand(0,len(input.data))\n",
        "        return input.mm(self.weight)\n",
        "\n",
        "\n",
        "class Sequential(Layer):\n",
        "    \n",
        "    def __init__(self, layers=list()):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layers = layers\n",
        "    \n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward(input)\n",
        "        return input\n",
        "    \n",
        "    def get_parameters(self):\n",
        "        params = list()\n",
        "        for l in self.layers:\n",
        "            params += l.get_parameters()\n",
        "        return params\n",
        "\n",
        "\n",
        "class Embedding(Layer):\n",
        "    \n",
        "    def __init__(self, vocab_size, dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.dim = dim\n",
        "        \n",
        "        # this random initialiation style is just a convention from word2vec\n",
        "        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)\n",
        "        \n",
        "        self.parameters.append(self.weight)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return self.weight.index_select(input)\n",
        "\n",
        "\n",
        "class Tanh(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return input.tanh()\n",
        "\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return input.sigmoid()\n",
        "    \n",
        "\n",
        "class CrossEntropyLoss(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, input, target):\n",
        "        return input.cross_entropy(target)\n",
        "\n",
        "    \n",
        "class RNNCell(Layer):\n",
        "    \n",
        "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output = n_output\n",
        "        \n",
        "        if(activation == 'sigmoid'):\n",
        "            self.activation = Sigmoid()\n",
        "        elif(activation == 'tanh'):\n",
        "            self.activation == Tanh()\n",
        "        else:\n",
        "            raise Exception(\"Non-linearity not found\")\n",
        "\n",
        "        self.w_ih = Linear(n_inputs, n_hidden)\n",
        "        self.w_hh = Linear(n_hidden, n_hidden)\n",
        "        self.w_ho = Linear(n_hidden, n_output)\n",
        "        \n",
        "        self.parameters += self.w_ih.get_parameters()\n",
        "        self.parameters += self.w_hh.get_parameters()\n",
        "        self.parameters += self.w_ho.get_parameters()        \n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        from_prev_hidden = self.w_hh.forward(hidden)\n",
        "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
        "        new_hidden = self.activation.forward(combined)\n",
        "        output = self.w_ho.forward(new_hidden)\n",
        "        return output, new_hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size=1):\n",
        "        return Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
        "    \n",
        "class LSTMCell(Layer):\n",
        "    \n",
        "    def __init__(self, n_inputs, n_hidden, n_output):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output = n_output\n",
        "\n",
        "        self.xf = Linear(n_inputs, n_hidden)\n",
        "        self.xi = Linear(n_inputs, n_hidden)\n",
        "        self.xo = Linear(n_inputs, n_hidden)        \n",
        "        self.xc = Linear(n_inputs, n_hidden)        \n",
        "        \n",
        "        self.hf = Linear(n_hidden, n_hidden, bias=False)\n",
        "        self.hi = Linear(n_hidden, n_hidden, bias=False)\n",
        "        self.ho = Linear(n_hidden, n_hidden, bias=False)\n",
        "        self.hc = Linear(n_hidden, n_hidden, bias=False)        \n",
        "        \n",
        "        self.w_ho = Linear(n_hidden, n_output, bias=False)\n",
        "        \n",
        "        self.parameters += self.xf.get_parameters()\n",
        "        self.parameters += self.xi.get_parameters()\n",
        "        self.parameters += self.xo.get_parameters()\n",
        "        self.parameters += self.xc.get_parameters()\n",
        "\n",
        "        self.parameters += self.hf.get_parameters()\n",
        "        self.parameters += self.hi.get_parameters()        \n",
        "        self.parameters += self.ho.get_parameters()        \n",
        "        self.parameters += self.hc.get_parameters()                \n",
        "        \n",
        "        self.parameters += self.w_ho.get_parameters()        \n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        \n",
        "        prev_hidden = hidden[0]        \n",
        "        prev_cell = hidden[1]\n",
        "        \n",
        "        f = (self.xf.forward(input) + self.hf.forward(prev_hidden)).sigmoid()\n",
        "        i = (self.xi.forward(input) + self.hi.forward(prev_hidden)).sigmoid()\n",
        "        o = (self.xo.forward(input) + self.ho.forward(prev_hidden)).sigmoid()        \n",
        "        g = (self.xc.forward(input) + self.hc.forward(prev_hidden)).tanh()        \n",
        "        c = (f * prev_cell) + (i * g)\n",
        "\n",
        "        h = o * c.tanh()\n",
        "        \n",
        "        output = self.w_ho.forward(h)\n",
        "        return output, (h, c)\n",
        "    \n",
        "    def init_hidden(self, batch_size=1):\n",
        "        init_hidden = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
        "        init_cell = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
        "        init_hidden.data[:,0] += 1\n",
        "        init_cell.data[:,0] += 1\n",
        "        return (init_hidden, init_cell)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faKNdkU8mYaO",
        "colab_type": "text"
      },
      "source": [
        "## The Tensor() class\n",
        "**The framework is based on the Tensor class, which can do:**\n",
        "  - Backpropagation <- possibly the most important part.\n",
        "  - Adding tensors `__add__`\n",
        "  - Inversing tensor sign. `__neg__`\n",
        "  - Substracting tensors `__sub__`\n",
        "  - Matrix multiplication `__mul__`\n",
        "  - Summing tensors `sum`\n",
        "    + Disambiguation: This function reduces a vector to a single value (the sum) in a dimension.\n",
        "  - expanding a Tensor `expand`\n",
        "    + Adds a dimension to a tensor\n",
        "  - transposing tensors `transpose`\n",
        "    + Transposes a tensor.\n",
        "  - Dot product `mm`\n",
        "    + Calculates dot product\n",
        "  - Sigmoid `sigmoid`\n",
        "    + Calculates sigmoid\n",
        "  - Tanh `tanh`\n",
        "   + Calculates tanh\n",
        "  - index_select `index_select`\n",
        "    + Selects the indices of the tensor\n",
        "  - softmax `softmax`\n",
        "    + Calculates softmax probabilities\n",
        "  - cross_entropy `cross_entropy`\n",
        "    + Calculates cross entropy\n",
        "  - repr `__repr__`\n",
        "    + returns the data of the Tensor as string\n",
        "  - string `__str__`\n",
        "    + returns the data of the Tensor as string\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzZmZGEexft9",
        "colab_type": "text"
      },
      "source": [
        "### The backward function (Backpropagation)\n",
        "This function is where the magic happens, and where the network learns.\n",
        "\n",
        "The steps of this function are:\n",
        "1. Check if the function has an origin (we cannot backpropagate the last parent nodes - eg, the imput data itself.)\n",
        "   + This throws an exception if you try to backpropagate more than once.\n",
        "2. If this is efectively the fathermost node, you add a gradient to it (\"The beggining of the road\")\n",
        "3. assert that gradients do not have gradients of their own.\n",
        "4. Check if there's something to backpropagate into AND if all gradients (From children) are accounted for, override waiting for children if \"backprop\" was called on this variable directly.\n",
        "  + Inside this part of the code we have all the backpropagation math for every single defined function.\n",
        "  + This functions aim to \"reverse\" the operations made in each function so you can \"standarize\" the values of the gradients in each layer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1Zp-ZfvmhTH",
        "colab_type": "text"
      },
      "source": [
        "### Testing every operation from the Tensor class. and examples of backpropagation with each operation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLh1ZLC0nLid",
        "colab_type": "text"
      },
      "source": [
        "#### add\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsPgVTOLnPrb",
        "colab_type": "code",
        "outputId": "cca4c3a5-0539-4bdc-b28f-774159f7fcd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "a = Tensor([1,1,1], autograd=True)\n",
        "b = Tensor([1,2,3], autograd=True)\n",
        "\n",
        "print(a.__add__(b))\n",
        "print(a + b) #Also works because __add__ overwrites the '+' (I think¿)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 3 4]\n",
            "[2 3 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbZ-4Qs_2mvc",
        "colab_type": "code",
        "outputId": "50b6c4b4-1802-4a90-f031-f49142d67312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "#The backpropagation\n",
        "c = a.__add__(b)\n",
        "print(c)\n",
        "print(c.grad)\n",
        "c.backward()\n",
        "print(c.grad) #The new calculated gradients (right now is 1 because we don't have a loss)\n",
        "  # and the backward() function assigns it automatically\n",
        "print(c.creators) #Where c comes from\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 3 4]\n",
            "None\n",
            "[1 1 1]\n",
            "[array([1, 1, 1]), array([1, 2, 3])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipnk78akn68V",
        "colab_type": "text"
      },
      "source": [
        "#### Neg\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VObr2nVNn9s_",
        "colab_type": "code",
        "outputId": "cedd1d64-3600-4fcd-8a30-419b92a73d50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "a = Tensor([1,1,1], autograd=True)\n",
        "a.__neg__()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1, -1, -1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2nQMTEY3PlV",
        "colab_type": "code",
        "outputId": "24b08a76-5ca0-4306-937d-993206a6bc51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "#The backpropagation\n",
        "c = a.__neg__()\n",
        "print(c)\n",
        "print(c.grad)\n",
        "c.backward()\n",
        "print(c.grad) #The new calculated gradients (right now is 1 because we don't have a loss)\n",
        "  # and the backward() function assigns it automatically\n",
        "print(c.creators) #Where c comes from"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1 -1 -1]\n",
            "None\n",
            "[1 1 1]\n",
            "[array([1, 1, 1])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU-7_HF8oBvE",
        "colab_type": "text"
      },
      "source": [
        "#### Sub\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB4T-vtUoDyh",
        "colab_type": "code",
        "outputId": "fdd2b384-22aa-4601-fb7a-40ab94e855f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "a = Tensor([1,1,1], autograd=True)\n",
        "b = Tensor([1,2,3], autograd=True)\n",
        "\n",
        "a.__sub__(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0, -1, -2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jap4GbV3ar6",
        "colab_type": "code",
        "outputId": "845fe226-d0c0-4ee1-96c4-c3fd2a6eceaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "#The backpropagation\n",
        "c = a.__sub__(b)\n",
        "print(c)\n",
        "print(c.grad)\n",
        "c.backward()\n",
        "print(c.grad) #The new calculated gradients (right now is 1 because we don't have a loss)\n",
        "  # and the backward() function assigns it automatically\n",
        "print(c.creators) #Where c comes from\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0 -1 -2]\n",
            "None\n",
            "[1 1 1]\n",
            "[array([1, 1, 1]), array([1, 2, 3])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tglCTmIroHWb",
        "colab_type": "text"
      },
      "source": [
        "#### Mul\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzmBip7aoK-t",
        "colab_type": "code",
        "outputId": "ce65cab0-6975-4a7e-b1ff-f18753cbcca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "a = Tensor([1,1,1], autograd=True)\n",
        "b = Tensor([1,2,3], autograd=True)\n",
        "\n",
        "a.__mul__(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01bycnCs3erq",
        "colab_type": "code",
        "outputId": "2e94a9f3-2cd1-4a8a-b32d-55862cdaa5f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "#The backpropagation\n",
        "c = a.__mul__(b)\n",
        "print(c)\n",
        "print(c.grad)\n",
        "c.backward()\n",
        "print(c.grad) #The new calculated gradients (right now is 1 because we don't have a loss)\n",
        "  # and the backward() function assigns it automatically\n",
        "print(c.creators) #Where c comes from\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3]\n",
            "None\n",
            "[1 1 1]\n",
            "[array([1, 1, 1]), array([1, 2, 3])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abY1fey73iL3",
        "colab_type": "text"
      },
      "source": [
        "Before continuing note that the magic of backprop is how you can combine functions, and stack them and still get the proper gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV5-HVDU3vcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = Tensor([1,1,1], autograd=True)\n",
        "b = Tensor([1,2,3], autograd=True)\n",
        "\n",
        "#The backpropagation\n",
        "c = a.__add__(b)\n",
        "d = b.__mul__(a)\n",
        "\n",
        "e = c.__sub__(d)\n",
        "f = e.__neg__()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiliW2nE6cm4",
        "colab_type": "code",
        "outputId": "e574dfdc-8649-48fe-df62-6ac8ef2a7e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(a.grad)\n",
        "print(a.children) #The id of the children nodes of a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "{585014643: 1, 551494389: 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrS4b_x14C2U",
        "colab_type": "code",
        "outputId": "c61e6755-1cd7-49d7-fc1a-911424da3640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "print(f)\n",
        "print(f.grad)\n",
        "f.backward()\n",
        "print(f.grad)\n",
        "print(f.creators) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1 -1 -1]\n",
            "None\n",
            "[1 1 1]\n",
            "[array([1, 1, 1])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWN6iQKC5wdV",
        "colab_type": "code",
        "outputId": "2a9f6188-9837-4757-9096-e911b86e54a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "### Also notice how the gradients of a and b also change when f is called\n",
        "  # Because f comes from a string of operations that start with a and b\n",
        "print(a)\n",
        "print(a.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1]\n",
            "[0 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D4Yypj2nGL3",
        "colab_type": "text"
      },
      "source": [
        "#### Sum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfVHOyYdj2cl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = Tensor([1,1,1], autograd = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcaJXcA0j33Z",
        "colab_type": "code",
        "outputId": "c6845fed-0cdf-44ee-f71b-bbd06dea3631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "a.sum(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKl4potb6pSE",
        "colab_type": "code",
        "outputId": "4ed30810-0f2c-477b-a0cb-55ffa7c678f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "#The backpropagation\n",
        "c = a.sum(0)\n",
        "print(c)\n",
        "print(c.grad)\n",
        "c.backward()\n",
        "print(c.grad)\n",
        "print(c.creators)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "None\n",
            "1\n",
            "[array([1, 1, 1])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nna7CUbUoOnS",
        "colab_type": "text"
      },
      "source": [
        "####Expand"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqA77jXOnHbc",
        "colab_type": "code",
        "outputId": "14a0bf9f-e7b9-4647-851a-33d55249f3c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "a = Tensor([1,1,1], autograd = True)\n",
        "print(a.expand(0,2)) #dimension to expand, copies\n",
        "print(\"\\n\")\n",
        "print(a.expand(1,4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 1]\n",
            " [1 1 1]]\n",
            "\n",
            "\n",
            "[[1 1 1 1]\n",
            " [1 1 1 1]\n",
            " [1 1 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0oyRKYS7nNd",
        "colab_type": "code",
        "outputId": "94ee7eae-877b-42f6-9d3f-75b15857ae2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "#The backpropagation\n",
        "c = a.expand(0,2)\n",
        "print(c)\n",
        "print(c.grad)\n",
        "c.backward()\n",
        "print(c.grad)\n",
        "print(c.creators)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 1]\n",
            " [1 1 1]]\n",
            "None\n",
            "[[1 1 1]\n",
            " [1 1 1]]\n",
            "[array([1, 1, 1])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WIu68AUoe0A",
        "colab_type": "text"
      },
      "source": [
        "#### Transpose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD2HxdH1oU-l",
        "colab_type": "code",
        "outputId": "49d0e89f-e22b-405b-fa58-d59ba869e3aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "a = Tensor([[0,0],[1,1],[2,2]], autograd = True)\n",
        "a.transpose()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 2],\n",
              "       [0, 1, 2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7T7SLfQ8BTw",
        "colab_type": "code",
        "outputId": "4f9ed180-03ee-4cc2-b25f-9fc6ef39b554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "#The backpropagation\n",
        "c = a.transpose()\n",
        "print(c)\n",
        "print(c.grad)\n",
        "c.backward()\n",
        "print(c.grad)\n",
        "print(c.creators)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 2]\n",
            " [0 1 2]]\n",
            "None\n",
            "[[1 1 1]\n",
            " [1 1 1]]\n",
            "[array([[0, 0],\n",
            "       [1, 1],\n",
            "       [2, 2]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghN5GiHwtFTi",
        "colab_type": "text"
      },
      "source": [
        "#### mm (dot product)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTBGsZZ3sJvC",
        "colab_type": "code",
        "outputId": "a02bee76-1581-4e1d-a795-e316f6448e77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "a = Tensor([1,1,1], autograd = True)\n",
        "b = Tensor([1,2,3], autograd = True)\n",
        "\n",
        "a.mm(b) #1*1 + 1*2 + 1*3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpFyKjAL8s8h",
        "colab_type": "code",
        "outputId": "0a2f0534-b888-4910-d789-d5d98e2d1df4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "#The backpropagation\n",
        "c = a.mm(b)\n",
        "print(c)\n",
        "print(c.grad)\n",
        "c.backward()\n",
        "print(c.grad)\n",
        "print(c.creators)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "None\n",
            "1\n",
            "[array([1, 1, 1]), array([1, 2, 3])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUgwgTRMtTWM",
        "colab_type": "text"
      },
      "source": [
        "#### Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NShD-CdstTD9",
        "colab_type": "code",
        "outputId": "ffc162bd-7664-4052-b5e3-a9d178f1c201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "a = Tensor([-1,1,3], autograd = True)\n",
        "a.sigmoid()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.26894142, 0.73105858, 0.95257413])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiXU_39j8221",
        "colab_type": "code",
        "outputId": "3617c84c-5c9b-4d67-e83e-faa39d7d4c56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "#The backpropagation\n",
        "c = a.sigmoid()\n",
        "print(c)\n",
        "print(c.grad)\n",
        "c.backward()\n",
        "print(c.grad)\n",
        "print(c.creators)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.26894142 0.73105858 0.95257413]\n",
            "None\n",
            "[1. 1. 1.]\n",
            "[array([-1,  1,  3])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjzTWmMataah",
        "colab_type": "text"
      },
      "source": [
        "####Tanh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkEal6WCtaCi",
        "colab_type": "code",
        "outputId": "38d70060-7815-4ad3-d3c6-532b379ebaad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "a = Tensor([-1,1,3], autograd = True)\n",
        "a.tanh()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.76159416,  0.76159416,  0.99505475])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIZZyJV786PK",
        "colab_type": "code",
        "outputId": "ea1088c9-d936-48e2-afb1-5a622b0001b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "#The backpropagation\n",
        "c = a.tanh()\n",
        "print(c)\n",
        "print(c.grad)\n",
        "c.backward()\n",
        "print(c.grad)\n",
        "print(c.creators)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.76159416  0.76159416  0.99505475]\n",
            "None\n",
            "[1. 1. 1.]\n",
            "[array([-1,  1,  3])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZw_HK68teW_",
        "colab_type": "text"
      },
      "source": [
        "####Index_select"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR8JtF3wtq4y",
        "colab_type": "code",
        "outputId": "8de23457-1d1e-4014-e4df-7f47acc3d47a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "a = Tensor([[0,0],[1,1],[2,2]], autograd = True)\n",
        "a.index_select(Tensor([0,2]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0],\n",
              "       [2, 2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuutpjzp8-He",
        "colab_type": "code",
        "outputId": "475f7bda-e588-43a1-9d26-e7d92aaf6b6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "#The backpropagation\n",
        "c = a.index_select(Tensor([0,2]))\n",
        "print(c)\n",
        "print(c.grad)\n",
        "c.backward()\n",
        "print(c.grad)\n",
        "print(c.creators)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0]\n",
            " [2 2]]\n",
            "None\n",
            "[[1 1]\n",
            " [1 1]]\n",
            "[array([[0, 0],\n",
            "       [1, 1],\n",
            "       [2, 2]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdjK6o2_vpDs",
        "colab_type": "text"
      },
      "source": [
        "#### Softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ry8F-WZvqdp",
        "colab_type": "code",
        "outputId": "905e36ac-3a4e-43e7-e904-093068c070db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "a = Tensor([[0,0],[10,1],[0,2]], autograd = True)\n",
        "a.softmax()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.00000000e-01, 5.00000000e-01],\n",
              "       [9.99876605e-01, 1.23394576e-04],\n",
              "       [1.19202922e-01, 8.80797078e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU_QAXyO9C0C",
        "colab_type": "code",
        "outputId": "3ed60ca2-b458-4e6d-dc56-3132538c7a58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "#The backpropagation\n",
        "  #Here there are no gradients, c.grads does not exist\n",
        "  # This is because it is a numpy array (because softmax transforms it)\n",
        "\"\"\"\n",
        "c = a.softmax()\n",
        "print(c)\n",
        "print(c.grad)\n",
        "c.backward()\n",
        "print(c.grad)\n",
        "print(c.creators)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nc = a.softmax()\\nprint(c)\\nprint(c.grad)\\nc.backward()\\nprint(c.grad)\\nprint(c.creators)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocHX9wMKvshp",
        "colab_type": "text"
      },
      "source": [
        "#### cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Pwf-QFJvr50",
        "colab_type": "code",
        "outputId": "5a1adb20-f103-4542-92d3-10e379e108e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "a = Tensor([[0,0],[10,1],[0,2]], autograd = True)\n",
        "targets = Tensor([0,1,0])\n",
        "a.cross_entropy(targets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(3.9400662)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Inu0tyAM9kYg",
        "colab_type": "code",
        "outputId": "f6c605f3-1af9-4c3c-9fe3-6a1d4ebe49d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "#The backpropagation\n",
        "c = a.cross_entropy(targets)\n",
        "print(c)\n",
        "print(c.grad)\n",
        "c.backward()\n",
        "print(c.grad)\n",
        "print(c.creators)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.9400661979308804\n",
            "None\n",
            "1.0\n",
            "[array([[ 0,  0],\n",
            "       [10,  1],\n",
            "       [ 0,  2]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKHMp_35v95i",
        "colab_type": "text"
      },
      "source": [
        "Some things must be commented on:\n",
        "If the tensor has more dimensions than the labels (eg previous example), you first sum them (to make them a single number), (same with the labels but you just flatten them) and then use the cross-entropy function:\n",
        "`loss = -(np.log(p) * (target_dist)).sum(1).mean()`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVD8MIc4w8aV",
        "colab_type": "text"
      },
      "source": [
        "#### Repr and string\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9Zcj_0kv5ZS",
        "colab_type": "code",
        "outputId": "41c77311-9c05-4c82-d6e4-f401cdc6d809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "a = Tensor([[0,0],[10,1],[0,2]], autograd = True)\n",
        "print(a.__repr__())\n",
        "print(a.__str__())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "array([[ 0,  0],\n",
            "       [10,  1],\n",
            "       [ 0,  2]])\n",
            "[[ 0  0]\n",
            " [10  1]\n",
            " [ 0  2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTiZTQEu9qDi",
        "colab_type": "text"
      },
      "source": [
        "## The Layer class\n",
        "Other classes were created to support our main class, they are\n",
        "  - Layer\n",
        "  - SGD\n",
        "  - Linear\n",
        "  - Sequential\n",
        "  - Embedding\n",
        "  - Tanh\n",
        "  - Sigmoid\n",
        "  - CrossEntropyLoss\n",
        "  - RNNCell\n",
        "  - LSTMCell\n",
        "\n",
        "The main function of every class (except Layer and SGD) is their respective .forward() function, which returns the prediction of that layer.\n",
        "\n",
        "For the Layer() class, it simply provides a base for every other class (Except SGD)\n",
        "\n",
        "For the SGD() class, it is an optimizer that applies the change in the weights to the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wxFQt9gATDX",
        "colab_type": "text"
      },
      "source": [
        "### Testing every class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5WcQZTqAYD8",
        "colab_type": "text"
      },
      "source": [
        "Layer()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdea2YAVxc3o",
        "colab_type": "code",
        "outputId": "87d394b9-edac-4372-f1bd-a6af9fcbffd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "Layer()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Layer at 0x7fc71de0f240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w-SDQCSQlrh",
        "colab_type": "text"
      },
      "source": [
        "#### SGD\n",
        "SGD Is an optimization layer (Stochastic gradient descent)\n",
        "To exemplify it a single pass example of a network will be given.\n",
        "\n",
        "(\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGR8UndOQqs3",
        "colab_type": "code",
        "outputId": "7a0d3a7a-ea1c-42b4-a463-4bb023f0df95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import numpy\n",
        "import sys\n",
        "\n",
        "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
        "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
        "\n",
        "w = list()\n",
        "w.append(Tensor(np.random.rand(2,3), autograd=True))\n",
        "w.append(Tensor(np.random.rand(3,1), autograd=True))\n",
        "\n",
        "optim = SGD(parameters=w, alpha=0.1)\n",
        "\n",
        "#for i in range(10): For multiple passes, uncomment\n",
        "\n",
        "# Predict\n",
        "pred = data.mm(w[0]).mm(w[1])\n",
        "\n",
        "# Compare\n",
        "loss = ((pred - target)*(pred - target)).sum(0)\n",
        "\n",
        "# Learn\n",
        "loss.backward(Tensor(np.ones_like(loss.data)))\n",
        "optim.step() #For every parameter, given a gradient, change the weights \n",
        "\n",
        "sys.stdout.write('\\r'+str(loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[0.58359915]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvYYL6LZRMBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U13-KmUAzp9",
        "colab_type": "text"
      },
      "source": [
        "#### Linear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay1DTDsrAo4l",
        "colab_type": "code",
        "outputId": "8e453fcb-3040-4b7c-a07f-84ad1914c4f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "Linear_layer = Linear(2,3)\n",
        "print(Linear_layer.weight)\n",
        "print(Linear_layer.bias)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.33626584 -1.03141487 -0.18351639]\n",
            " [ 0.06167598  0.35537624  2.04473991]]\n",
            "[0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzjpUDbuBG6q",
        "colab_type": "code",
        "outputId": "92e4a93c-d30f-42a8-dd20-292650303c46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Prediction, it's the weights \n",
        "Linear_layer.forward(Tensor([1,2],autograd=True))\n",
        "#Returns the same number because it is dot-product"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.68639651, -2.47933601, -0.86747018],\n",
              "       [-1.68639651, -2.47933601, -0.86747018]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3c47TesBD97",
        "colab_type": "text"
      },
      "source": [
        "#### Sequential"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W3Gjx3NB13g",
        "colab_type": "code",
        "outputId": "ad1db8ac-9190-4c43-f71d-e7b832bd6374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "Sequential_layer = Sequential([Linear(2,3), Linear(3,1)])\n",
        "SLParams = Sequential_layer.get_parameters()\n",
        "SLParams[0:1] #Weights and biases of first linear (with second would be the next ones)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 2.32571573,  2.19716518, -1.18415004],\n",
              "        [-0.76148084, -0.11429519,  1.45228121]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkVOU7zaC3_8",
        "colab_type": "code",
        "outputId": "1a31bb45-6fe2-46b0-b366-b0592d46e37e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#NOTE THAT THE DIMENSIONS MUST MAKE SENSE, the tensor passed has 1x2 -> \n",
        "  # 2x3 (of first linear) ->3x1 (returns a single value per input)\n",
        "Sequential_layer.forward(Tensor([1,0],autograd=True)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.74233009],\n",
              "       [-0.74233009]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqetPnSRBs6f",
        "colab_type": "text"
      },
      "source": [
        "#### Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw9cmgwBC16-",
        "colab_type": "code",
        "outputId": "6f8b05ef-da05-4cf8-a476-7a03c4b36524",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "Embedding_layer = Embedding(2,3)\n",
        "\n",
        "\n",
        "#Parameters show all the parameters, in this case, the layer only has weights\n",
        "print(Embedding_layer.parameters)\n",
        "print(\"\\n\")\n",
        "Embedding_layer.forward(Tensor([1,0],autograd=True)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[ 0.05573594,  0.10320427, -0.01470895],\n",
            "       [ 0.01508788, -0.08155138,  0.02532749]])]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.01508788, -0.08155138,  0.02532749],\n",
              "       [ 0.05573594,  0.10320427, -0.01470895]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzsXNG8IDm1A",
        "colab_type": "text"
      },
      "source": [
        "#### Tanh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ty6BTj_DmXM",
        "colab_type": "code",
        "outputId": "2aa2e310-43f3-4897-f214-f92069fc2744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "tanh_layer = Tanh()\n",
        "tanh_layer.forward(Tensor([-1,2,3],autograd = True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.76159416,  0.96402758,  0.99505475])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2DQKuPZJrKO",
        "colab_type": "text"
      },
      "source": [
        "#### Sigmoid\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agYMX2t0Jqo-",
        "colab_type": "code",
        "outputId": "9953969d-afd6-483c-b148-2494458dc225",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "sigmoid_layer = Sigmoid()\n",
        "sigmoid_layer.forward(Tensor([1,2,3],autograd = True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.73105858, 0.88079708, 0.95257413])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi9h_q0AJwZi",
        "colab_type": "text"
      },
      "source": [
        "#### Cross entropy loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mjI2HVBJv_m",
        "colab_type": "code",
        "outputId": "b2d4cc97-059f-4516-c0e0-95faf1614c6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "CrossEntopy_layer = CrossEntropyLoss()\n",
        "CrossEntopy_layer.forward(Tensor([1,2,3],autograd = True),Tensor([1],autograd = True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(1.40760596)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNPUiKsfJ_wj",
        "colab_type": "text"
      },
      "source": [
        "#### RNNCell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH2GIAVJJ8_L",
        "colab_type": "code",
        "outputId": "94437bed-87ab-40bd-8f4c-4482a1b7073c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "#n_inputs must match with the second size of the embedding layer\n",
        "model = RNNCell(n_inputs=3, n_hidden=2, n_output=1)\n",
        "hidden = model.init_hidden(batch_size = 1)\n",
        "rnn_input = Embedding(2,3).forward(Tensor([1,0],autograd=True)) #Taken from the embeding class \n",
        "## rnn_input is 2x3\n",
        "output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
        "print(hidden)\n",
        "print(\"\\n\")\n",
        "print(output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.49790149 0.51723242]\n",
            " [0.56408507 0.50974354]]\n",
            "\n",
            "\n",
            "[[-0.02591581]\n",
            " [-0.02032943]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70DdrR-RLeOZ",
        "colab_type": "text"
      },
      "source": [
        "#### LSTM Cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRw4L2RiKq6J",
        "colab_type": "code",
        "outputId": "f7af7a45-d5a1-4bc0-ac10-6cd56b8ed301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "current_vocab_size = 5\n",
        "input_dimension = 3\n",
        "#dim has to be equal to n_inputs\n",
        "embed_lstm = Embedding(vocab_size=current_vocab_size,dim=input_dimension)\n",
        "model = LSTMCell(n_inputs=input_dimension, n_hidden=4, n_output=current_vocab_size)\n",
        "\n",
        "#n_hidden is the number of hidden layers (and weights).\n",
        "hidden = model.init_hidden(batch_size = 3) #Has to be multiple(broadcastalbe) of the n_iputs X n_hidden\n",
        "lstm_input = embed_lstm.forward(Tensor([1,0,1],autograd=True))\n",
        "# THE OUTPUT DEPENDS ON THE INPUT DIMENSION, WHICH HAS TO BE MULTIPLE OF BATCH_SIZE\n",
        "  # AND HAS TO MAKE SENSE WITH THE SHAPE OF THE INPUT TENSOR OF THE EMBED_LSTM layer\n",
        "output, hidden = model.forward(input=lstm_input, hidden=hidden)\n",
        "#Output is vocab_size X input_dimension (basically the predictions for the input tensor)\n",
        "print(hidden) #The hidden layers\n",
        "print(\"\\n\")\n",
        "print(output)#The predictions ()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([[ 0.20474022, -0.22035093, -0.22545262, -0.03007398],\n",
            "       [ 0.20072954, -0.22994678, -0.22875616, -0.0770633 ],\n",
            "       [ 0.20474022, -0.22035093, -0.22545262, -0.03007398]]), array([[ 0.62935339, -0.43628705, -0.54277915, -0.04061711],\n",
            "       [ 0.58501956, -0.48393447, -0.54249725, -0.10305764],\n",
            "       [ 0.62935339, -0.43628705, -0.54277915, -0.04061711]]))\n",
            "\n",
            "\n",
            "[[-0.49274267 -0.21660056  0.22842868 -0.01471593 -0.20429764]\n",
            " [-0.47992161 -0.30172723  0.21678575 -0.015754   -0.23023531]\n",
            " [-0.49274267 -0.21660056  0.22842868 -0.01471593 -0.20429764]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLzvtzYDNPCK",
        "colab_type": "code",
        "outputId": "9768cab2-eaeb-4e1f-a6db-6ecb76f29df0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "embed_lstm.parameters #returns vocab_size array X input_dimension"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.14687184,  0.12223735, -0.10779355],\n",
              "        [-0.16010357,  0.00816488, -0.00171421],\n",
              "        [-0.14608273, -0.16573372, -0.03558867],\n",
              "        [ 0.05641243, -0.00957594,  0.00776743],\n",
              "        [ 0.06203665,  0.05508864, -0.09712408]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 318
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT-FUX9OMNE9",
        "colab_type": "code",
        "outputId": "b931e601-41a7-4cfe-e0d6-b3a9ffd15528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "np.array(output.data).shape # (, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 319
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n24_s_bqOuy2",
        "colab_type": "text"
      },
      "source": [
        "----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPdkewHlOsC8",
        "colab_type": "text"
      },
      "source": [
        "# Training and using the mini-framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVrJ1XWQPvq8",
        "colab_type": "text"
      },
      "source": [
        "## Example and data taken from Grokking Deep Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rdyqqYoOuRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys,random,math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# dataset from http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "# (Can be found in repo as well)\n",
        "\n",
        "#Load data\n",
        "f = open('shakespear.txt','r')\n",
        "raw = f.read()\n",
        "f.close()\n",
        "\n",
        "# Pre-process data\n",
        "vocab = list(set(raw))\n",
        "word2index = {}\n",
        "for i,word in enumerate(vocab):\n",
        "    word2index[word]=i\n",
        "indices = np.array(list(map(lambda x:word2index[x], raw)))\n",
        "\n",
        "#Set the layers and mode\n",
        "embed = Embedding(vocab_size=len(vocab),dim=512)\n",
        "model = LSTMCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
        "model.w_ho.weight.data *= 0\n",
        "\n",
        "#Set the criterion (loss) and start the optimizer\n",
        "criterion = CrossEntropyLoss()\n",
        "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)\n",
        "\n",
        "# This function  generates an example of text predicted with the network\n",
        "def generate_sample(n=30, init_char=' '):\n",
        "    s = \"\"\n",
        "    hidden = model.init_hidden(batch_size=1)\n",
        "    input = Tensor(np.array([word2index[init_char]]))\n",
        "    for i in range(n):\n",
        "        rnn_input = embed.forward(input)\n",
        "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
        "        output.data *= 15\n",
        "        temp_dist = output.softmax()\n",
        "        temp_dist /= temp_dist.sum()\n",
        "\n",
        "#         m = (temp_dist > np.random.rand()).argmax() # sample from predictions\n",
        "        m = output.data.argmax() # take the max prediction\n",
        "        c = vocab[m]\n",
        "        input = Tensor(np.array([m]))\n",
        "        s += c\n",
        "    return s\n",
        "\n",
        "#set batch_size\n",
        "batch_size = 16\n",
        "bptt = 25\n",
        "n_batches = int((indices.shape[0] / (batch_size)))\n",
        "\n",
        "#Indices is the whole data as ints\n",
        "# Separating the indices into batches\n",
        "# This part of the code is very important\n",
        "#This line reshapes the dataset so each column is a section of the initial indices array\n",
        "#The issue is that the previous line prints the characters vertically (not horizontally)\n",
        "#So you have to transpose it to fix this\n",
        "trimmed_indices = indices[:n_batches*batch_size]\n",
        "batched_indices = trimmed_indices.reshape(batch_size, n_batches).transpose()\n",
        "\n",
        "#Takes every batch except the last one\n",
        "input_batched_indices = batched_indices[0:-1]\n",
        "#Takes every batch except the first one.\n",
        "target_batched_indices = batched_indices[1:]\n",
        "\n",
        "#Get the number of truncated batches to use.\n",
        "n_bptt = int(((n_batches-1) / bptt))\n",
        "#Pass the batches to useful shapes\n",
        "input_batches = input_batched_indices[:n_bptt*bptt]\n",
        "input_batches = input_batches.reshape(n_bptt,bptt,batch_size)\n",
        "\n",
        "target_batches = target_batched_indices[:n_bptt*bptt]\n",
        "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvJUSC7WTKZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The actual training\n",
        "def train(iterations=400):\n",
        "    min_loss = 1000\n",
        "    #For every iteration\n",
        "    for iter in range(iterations):\n",
        "        #Set total loss and n_loss to zero\n",
        "        total_loss = 0\n",
        "        n_loss = 0\n",
        "        \n",
        "        #Start the hidden model\n",
        "        hidden = model.init_hidden(batch_size=batch_size)\n",
        "        # Get the length of batches to train\n",
        "        batches_to_train = len(input_batches)\n",
        "        # batches_to_train = 32\n",
        "        \n",
        "        # For every batch do:\n",
        "        for batch_i in range(batches_to_train):\n",
        "            \n",
        "            #Set the hidden layers as tensors\n",
        "            hidden = (Tensor(hidden[0].data, autograd=True), Tensor(hidden[1].data, autograd=True))\n",
        "            \n",
        "            #Define the losses as a list.\n",
        "            losses = list()\n",
        "            \n",
        "            #For every example in bptt (truncated backprop)\n",
        "            for t in range(bptt):\n",
        "              # We have batches of batches (195,16,32): 195 batches of batches with 32 values each\n",
        "              #The input is the 32 values of current bptt batch\n",
        "              input = Tensor(input_batches[batch_i][t], autograd=True)\n",
        "              #embed the input\n",
        "              rnn_input = embed.forward(input=input)\n",
        "              #Pass the embedded inputs to the model and the hidden values as well\n",
        "                #Get the prediction\n",
        "              output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
        "              \n",
        "              #Get the targets, get the current batch loss\n",
        "              target = Tensor(target_batches[batch_i][t], autograd=True)    \n",
        "              batch_loss = criterion.forward(output, target)\n",
        "\n",
        "              #Append the batch loss to the losses list\n",
        "              if(t == 0):\n",
        "                  losses.append(batch_loss)\n",
        "              #if this is not the first batch, sum the current loss and the batch_loss\n",
        "                #losses is an array of accumulated losses from the first batch to the last one of bptt\n",
        "              else:\n",
        "                  losses.append(batch_loss + losses[-1])\n",
        "\n",
        "            #ONCE YOU HAVE FORWARDPROPAGATED THROUGH ALL THE BPTT BATCHES\n",
        "            #Take the last value of losses\n",
        "            loss = losses[-1]\n",
        "            \n",
        "\n",
        "            #Backpropagate.\n",
        "            loss.backward()\n",
        "            #Optimize\n",
        "            optim.step()\n",
        "            #Add the regularized bptt loss to the total loss\n",
        "            total_loss += loss.data / bptt\n",
        "            \n",
        "            #Set the epoch loss\n",
        "            epoch_loss = np.exp(total_loss / (batch_i+1))\n",
        "            #If we made a 'new record' set it\n",
        "            if(epoch_loss < min_loss):\n",
        "                min_loss = epoch_loss\n",
        "\n",
        "            #Set metrics\n",
        "            log = \"\\r Iter:\" + str(iter)\n",
        "            log += \" - Alpha:\" + str(optim.alpha)[0:5]\n",
        "            log += \" - Batch \"+str(batch_i+1)+\"/\"+str(len(input_batches))\n",
        "            log += \" - Min Loss:\" + str(min_loss)[0:5]\n",
        "            log += \" - Loss:\" + str(epoch_loss)\n",
        "            #For every batch, print the logs and a sample\n",
        "            if(batch_i == 0):\n",
        "                log += \" - \" + generate_sample(n=70, init_char='T').replace(\"\\n\",\" \")\n",
        "            if(batch_i % 1 == 0):\n",
        "                sys.stdout.write(log)\n",
        "        optim.alpha *= 0.99"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNwWBGCTTRgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}